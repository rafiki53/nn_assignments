{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "**Submission deadline: last lab session before or on Tuesday, 09.5.17**\n",
    "\n",
    "**Points: 9 + 10 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "For programming exerciese add your solutions to the notebook. For math exercies please provide us with answers on paper or type them in the notebook (it supports Latex-like equations).\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 28 days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "This assignment builds on code from Assignment 4, Problem 7. \n",
    "For your convenience, we have copied the code below. Please copy your solution from the old list, or fill in the blanks below to get a working network.\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code - many network implementations follow a similar pattern.\n",
    "\n",
    "Please make sure that the network trains to nearly 100% accuracy on Iris.\n",
    "\n",
    "## Task\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below **2% testing errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            weight_init = IsotropicGaussian(0.2, 0.0)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        #fprop_context['Y'] = Y\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        #print(fprop_context['Y'].shape, dLdY.shape, self.W.shape)\n",
    "        dLdX = np.dot(self.W.T, dLdY)\n",
    "        #X = fprop_context['X']\n",
    "        #print(X.shape, dLdX.shape)\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        return dLdY * (1 - Y ** 2)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 0.952307, train error rate 35.333333%\n",
      "At iteration 1000, loss 0.052332, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.043688, train error rate 1.333333%\n",
      "At iteration 3000, loss 0.041233, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.039979, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.039107, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.038422, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.037855, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.037376, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.036958, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.036579, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.036216, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.035849, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.035465, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.035057, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.034620, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.034156, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.033667, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.033154, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.032620, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.032065, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.031490, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.030896, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.030279, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.029633, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.028947, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.028201, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.027358, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.026361, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.025177, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.023852, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.022449, train error rate 1.333333%\n",
      "At iteration 32000, loss 0.021001, train error rate 1.333333%\n",
      "At iteration 33000, loss 0.019529, train error rate 0.666667%\n",
      "At iteration 34000, loss 0.018055, train error rate 0.666667%\n",
      "At iteration 35000, loss 0.016600, train error rate 0.666667%\n",
      "At iteration 36000, loss 0.015186, train error rate 0.666667%\n",
      "At iteration 37000, loss 0.013832, train error rate 0.666667%\n",
      "At iteration 38000, loss 0.012558, train error rate 0.000000%\n",
      "At iteration 39000, loss 0.011385, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.010324, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.009378, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.008544, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.007812, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.007169, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.006605, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.006108, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.005668, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.005278, train error rate 0.000000%\n",
      "At iteration 49000, loss 0.004931, train error rate 0.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [4p]\n",
    "\n",
    "Implement the following additions to the SGD code below:\n",
    "1. Momentum [2p]\n",
    "2. Learning rate schedule [1p]\n",
    "3. Weight decay [1p]. One way to implement it is to use the functions `net.params` and `net.param_names` to get all parameters whose names are \"W\" and not \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            alpha = 5e-2\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    if N=='W':\n",
    "                        pass\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        #G += TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    alpha *= 0.995\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    # epsilon = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    # V[...] = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -alpha * G\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [5p]\n",
    "\n",
    "Tune the following network to reach below 1.9% error rate on\n",
    "the validation set. This should result in a test error below 2%. To\n",
    "tune the network you will need to:\n",
    "1. choose the number of layers (more than 1, less than 5),\n",
    "2. choose the number of neurons in each layer (more than 100,\n",
    "    less than 5000),\n",
    "3. pick proper weight initialization,\n",
    "4. pick proper learning rate schedule (need to decay over time,\n",
    "    good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
    "    half of that after 10000 batches),\n",
    "5. pick a momentum constant (probably a constant one will be OK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 0.873904, batch error rate 27.000000%\n",
      "At minibatch 200, batch loss 1.118459, batch error rate 31.000000%\n",
      "At minibatch 300, batch loss 0.747644, batch error rate 18.000000%\n",
      "At minibatch 400, batch loss 0.787446, batch error rate 25.000000%\n",
      "At minibatch 500, batch loss 0.654459, batch error rate 20.000000%\n",
      "After epoch 1: valid_err_rate: 0.217600% currently going ot do 3 epochs\n",
      "At minibatch 600, batch loss 0.481277, batch error rate 15.000000%\n",
      "At minibatch 700, batch loss 0.781308, batch error rate 20.000000%\n",
      "At minibatch 800, batch loss 0.609309, batch error rate 23.000000%\n",
      "At minibatch 900, batch loss 0.615990, batch error rate 17.000000%\n",
      "At minibatch 1000, batch loss 0.645604, batch error rate 22.000000%\n",
      "After epoch 2: valid_err_rate: 0.161700% currently going ot do 4 epochs\n",
      "At minibatch 1100, batch loss 0.708365, batch error rate 22.000000%\n",
      "At minibatch 1200, batch loss 0.539726, batch error rate 20.000000%\n",
      "At minibatch 1300, batch loss 0.569375, batch error rate 17.000000%\n",
      "At minibatch 1400, batch loss 0.423309, batch error rate 16.000000%\n",
      "At minibatch 1500, batch loss 0.557465, batch error rate 20.000000%\n",
      "After epoch 3: valid_err_rate: 0.139800% currently going ot do 5 epochs\n",
      "At minibatch 1600, batch loss 0.553214, batch error rate 19.000000%\n",
      "At minibatch 1700, batch loss 0.506493, batch error rate 15.000000%\n",
      "At minibatch 1800, batch loss 0.564215, batch error rate 12.000000%\n",
      "At minibatch 1900, batch loss 0.646276, batch error rate 24.000000%\n",
      "At minibatch 2000, batch loss 0.312948, batch error rate 11.000000%\n",
      "After epoch 4: valid_err_rate: 0.128400% currently going ot do 7 epochs\n",
      "At minibatch 2100, batch loss 0.561857, batch error rate 19.000000%\n",
      "At minibatch 2200, batch loss 0.553472, batch error rate 15.000000%\n",
      "At minibatch 2300, batch loss 0.338056, batch error rate 11.000000%\n",
      "At minibatch 2400, batch loss 0.653443, batch error rate 13.000000%\n",
      "At minibatch 2500, batch loss 0.358228, batch error rate 16.000000%\n",
      "After epoch 5: valid_err_rate: 0.120000% currently going ot do 8 epochs\n",
      "At minibatch 2600, batch loss 0.300893, batch error rate 8.000000%\n",
      "At minibatch 2700, batch loss 0.401801, batch error rate 11.000000%\n",
      "At minibatch 2800, batch loss 0.646823, batch error rate 12.000000%\n",
      "At minibatch 2900, batch loss 0.485975, batch error rate 11.000000%\n",
      "At minibatch 3000, batch loss 0.408411, batch error rate 13.000000%\n",
      "After epoch 6: valid_err_rate: 0.115100% currently going ot do 10 epochs\n",
      "At minibatch 3100, batch loss 0.511847, batch error rate 11.000000%\n",
      "At minibatch 3200, batch loss 0.389586, batch error rate 15.000000%\n",
      "At minibatch 3300, batch loss 0.428108, batch error rate 13.000000%\n",
      "At minibatch 3400, batch loss 0.356870, batch error rate 10.000000%\n",
      "At minibatch 3500, batch loss 0.445216, batch error rate 13.000000%\n",
      "After epoch 7: valid_err_rate: 0.111100% currently going ot do 11 epochs\n",
      "At minibatch 3600, batch loss 0.846677, batch error rate 22.000000%\n",
      "At minibatch 3700, batch loss 0.398917, batch error rate 15.000000%\n",
      "At minibatch 3800, batch loss 0.315123, batch error rate 14.000000%\n",
      "At minibatch 3900, batch loss 0.423660, batch error rate 14.000000%\n",
      "At minibatch 4000, batch loss 0.549055, batch error rate 15.000000%\n",
      "After epoch 8: valid_err_rate: 0.106800% currently going ot do 13 epochs\n",
      "At minibatch 4100, batch loss 0.281963, batch error rate 9.000000%\n",
      "At minibatch 4200, batch loss 0.526137, batch error rate 10.000000%\n",
      "At minibatch 4300, batch loss 0.498966, batch error rate 9.000000%\n",
      "At minibatch 4400, batch loss 0.325413, batch error rate 10.000000%\n",
      "At minibatch 4500, batch loss 0.479458, batch error rate 12.000000%\n",
      "After epoch 9: valid_err_rate: 0.104400% currently going ot do 14 epochs\n",
      "At minibatch 4600, batch loss 0.191950, batch error rate 6.000000%\n",
      "At minibatch 4700, batch loss 0.366173, batch error rate 10.000000%\n",
      "At minibatch 4800, batch loss 0.350827, batch error rate 14.000000%\n",
      "At minibatch 4900, batch loss 0.521176, batch error rate 13.000000%\n",
      "At minibatch 5000, batch loss 0.227834, batch error rate 7.000000%\n",
      "After epoch 10: valid_err_rate: 0.101700% currently going ot do 16 epochs\n",
      "At minibatch 5100, batch loss 0.410997, batch error rate 12.000000%\n",
      "At minibatch 5200, batch loss 0.357220, batch error rate 10.000000%\n",
      "At minibatch 5300, batch loss 0.454058, batch error rate 17.000000%\n",
      "At minibatch 5400, batch loss 0.285846, batch error rate 9.000000%\n",
      "At minibatch 5500, batch loss 0.327838, batch error rate 11.000000%\n",
      "After epoch 11: valid_err_rate: 0.098900% currently going ot do 17 epochs\n",
      "At minibatch 5600, batch loss 0.288303, batch error rate 13.000000%\n",
      "At minibatch 5700, batch loss 0.276889, batch error rate 8.000000%\n",
      "At minibatch 5800, batch loss 0.317030, batch error rate 10.000000%\n",
      "At minibatch 5900, batch loss 0.452786, batch error rate 15.000000%\n",
      "At minibatch 6000, batch loss 0.427018, batch error rate 12.000000%\n",
      "After epoch 12: valid_err_rate: 0.098600% currently going ot do 19 epochs\n",
      "At minibatch 6100, batch loss 0.296915, batch error rate 7.000000%\n",
      "At minibatch 6200, batch loss 0.497402, batch error rate 14.000000%\n",
      "At minibatch 6300, batch loss 0.448109, batch error rate 13.000000%\n",
      "At minibatch 6400, batch loss 0.314840, batch error rate 8.000000%\n",
      "At minibatch 6500, batch loss 0.319464, batch error rate 9.000000%\n",
      "After epoch 13: valid_err_rate: 0.095400% currently going ot do 20 epochs\n",
      "At minibatch 6600, batch loss 0.536404, batch error rate 14.000000%\n",
      "At minibatch 6700, batch loss 0.321988, batch error rate 10.000000%\n",
      "At minibatch 6800, batch loss 0.355414, batch error rate 12.000000%\n",
      "At minibatch 6900, batch loss 0.273155, batch error rate 10.000000%\n",
      "At minibatch 7000, batch loss 0.579411, batch error rate 13.000000%\n",
      "After epoch 14: valid_err_rate: 0.095200% currently going ot do 22 epochs\n",
      "At minibatch 7100, batch loss 0.554034, batch error rate 14.000000%\n",
      "At minibatch 7200, batch loss 0.386045, batch error rate 9.000000%\n",
      "At minibatch 7300, batch loss 0.424236, batch error rate 14.000000%\n",
      "At minibatch 7400, batch loss 0.422061, batch error rate 9.000000%\n",
      "At minibatch 7500, batch loss 0.359815, batch error rate 11.000000%\n",
      "After epoch 15: valid_err_rate: 0.094800% currently going ot do 23 epochs\n",
      "At minibatch 7600, batch loss 0.236939, batch error rate 9.000000%\n",
      "At minibatch 7700, batch loss 0.416447, batch error rate 10.000000%\n",
      "At minibatch 7800, batch loss 0.498712, batch error rate 18.000000%\n",
      "At minibatch 7900, batch loss 0.450503, batch error rate 11.000000%\n",
      "At minibatch 8000, batch loss 0.285363, batch error rate 6.000000%\n",
      "After epoch 16: valid_err_rate: 0.093200% currently going ot do 25 epochs\n",
      "At minibatch 8100, batch loss 0.479261, batch error rate 15.000000%\n",
      "At minibatch 8200, batch loss 0.401077, batch error rate 9.000000%\n",
      "At minibatch 8300, batch loss 0.437628, batch error rate 13.000000%\n",
      "At minibatch 8400, batch loss 0.401712, batch error rate 10.000000%\n",
      "At minibatch 8500, batch loss 0.242761, batch error rate 8.000000%\n",
      "After epoch 17: valid_err_rate: 0.091800% currently going ot do 26 epochs\n",
      "At minibatch 8600, batch loss 0.383866, batch error rate 11.000000%\n",
      "At minibatch 8700, batch loss 0.377654, batch error rate 10.000000%\n",
      "At minibatch 8800, batch loss 0.388908, batch error rate 9.000000%\n",
      "At minibatch 8900, batch loss 0.276534, batch error rate 7.000000%\n",
      "At minibatch 9000, batch loss 0.493660, batch error rate 12.000000%\n",
      "After epoch 18: valid_err_rate: 0.092400% currently going ot do 26 epochs\n",
      "At minibatch 9100, batch loss 0.418129, batch error rate 12.000000%\n",
      "At minibatch 9200, batch loss 0.477293, batch error rate 13.000000%\n",
      "At minibatch 9300, batch loss 0.247935, batch error rate 6.000000%\n",
      "At minibatch 9400, batch loss 0.175344, batch error rate 3.000000%\n",
      "At minibatch 9500, batch loss 0.305114, batch error rate 13.000000%\n",
      "After epoch 19: valid_err_rate: 0.091000% currently going ot do 29 epochs\n",
      "At minibatch 9600, batch loss 0.652650, batch error rate 15.000000%\n",
      "At minibatch 9700, batch loss 0.188139, batch error rate 6.000000%\n",
      "At minibatch 9800, batch loss 0.207040, batch error rate 4.000000%\n",
      "At minibatch 9900, batch loss 0.319185, batch error rate 9.000000%\n",
      "At minibatch 10000, batch loss 0.126614, batch error rate 5.000000%\n",
      "After epoch 20: valid_err_rate: 0.090400% currently going ot do 31 epochs\n",
      "At minibatch 10100, batch loss 0.224643, batch error rate 10.000000%\n",
      "At minibatch 10200, batch loss 0.275461, batch error rate 7.000000%\n",
      "At minibatch 10300, batch loss 0.526965, batch error rate 14.000000%\n",
      "At minibatch 10400, batch loss 0.416536, batch error rate 11.000000%\n",
      "At minibatch 10500, batch loss 0.413447, batch error rate 8.000000%\n",
      "After epoch 21: valid_err_rate: 0.089800% currently going ot do 32 epochs\n",
      "At minibatch 10600, batch loss 0.359628, batch error rate 12.000000%\n",
      "At minibatch 10700, batch loss 0.394051, batch error rate 12.000000%\n",
      "At minibatch 10800, batch loss 0.249607, batch error rate 9.000000%\n",
      "At minibatch 10900, batch loss 0.407903, batch error rate 10.000000%\n",
      "At minibatch 11000, batch loss 0.359594, batch error rate 9.000000%\n",
      "After epoch 22: valid_err_rate: 0.088500% currently going ot do 34 epochs\n",
      "At minibatch 11100, batch loss 0.254439, batch error rate 8.000000%\n",
      "At minibatch 11200, batch loss 0.354532, batch error rate 11.000000%\n",
      "At minibatch 11300, batch loss 0.369524, batch error rate 10.000000%\n",
      "At minibatch 11400, batch loss 0.152302, batch error rate 4.000000%\n",
      "At minibatch 11500, batch loss 0.399991, batch error rate 12.000000%\n",
      "After epoch 23: valid_err_rate: 0.089000% currently going ot do 34 epochs\n",
      "At minibatch 11600, batch loss 0.292467, batch error rate 9.000000%\n",
      "At minibatch 11700, batch loss 0.387006, batch error rate 11.000000%\n",
      "At minibatch 11800, batch loss 0.168612, batch error rate 3.000000%\n",
      "At minibatch 11900, batch loss 0.434137, batch error rate 13.000000%\n",
      "At minibatch 12000, batch loss 0.193507, batch error rate 6.000000%\n",
      "After epoch 24: valid_err_rate: 0.088500% currently going ot do 34 epochs\n",
      "At minibatch 12100, batch loss 0.236187, batch error rate 6.000000%\n",
      "At minibatch 12200, batch loss 0.494023, batch error rate 14.000000%\n",
      "At minibatch 12300, batch loss 0.334433, batch error rate 10.000000%\n",
      "At minibatch 12400, batch loss 0.439071, batch error rate 10.000000%\n",
      "At minibatch 12500, batch loss 0.345872, batch error rate 10.000000%\n",
      "After epoch 25: valid_err_rate: 0.087600% currently going ot do 38 epochs\n",
      "At minibatch 12600, batch loss 0.403068, batch error rate 8.000000%\n",
      "At minibatch 12700, batch loss 0.121117, batch error rate 4.000000%\n",
      "At minibatch 12800, batch loss 0.338408, batch error rate 13.000000%\n",
      "At minibatch 12900, batch loss 0.554015, batch error rate 17.000000%\n",
      "At minibatch 13000, batch loss 0.346866, batch error rate 9.000000%\n",
      "After epoch 26: valid_err_rate: 0.087400% currently going ot do 40 epochs\n",
      "At minibatch 13100, batch loss 0.435691, batch error rate 9.000000%\n",
      "At minibatch 13200, batch loss 0.233611, batch error rate 9.000000%\n",
      "At minibatch 13300, batch loss 0.267206, batch error rate 9.000000%\n",
      "At minibatch 13400, batch loss 0.301450, batch error rate 9.000000%\n",
      "At minibatch 13500, batch loss 0.282937, batch error rate 9.000000%\n",
      "After epoch 27: valid_err_rate: 0.087300% currently going ot do 41 epochs\n",
      "At minibatch 13600, batch loss 0.355676, batch error rate 12.000000%\n",
      "At minibatch 13700, batch loss 0.465107, batch error rate 12.000000%\n",
      "At minibatch 13800, batch loss 0.255658, batch error rate 8.000000%\n",
      "At minibatch 13900, batch loss 0.363518, batch error rate 9.000000%\n",
      "At minibatch 14000, batch loss 0.434952, batch error rate 8.000000%\n",
      "After epoch 28: valid_err_rate: 0.086700% currently going ot do 43 epochs\n",
      "At minibatch 14100, batch loss 0.205883, batch error rate 6.000000%\n",
      "At minibatch 14200, batch loss 0.189084, batch error rate 4.000000%\n",
      "At minibatch 14300, batch loss 0.330527, batch error rate 12.000000%\n",
      "At minibatch 14400, batch loss 0.280266, batch error rate 10.000000%\n",
      "At minibatch 14500, batch loss 0.341144, batch error rate 10.000000%\n",
      "After epoch 29: valid_err_rate: 0.086500% currently going ot do 44 epochs\n",
      "At minibatch 14600, batch loss 0.388492, batch error rate 9.000000%\n",
      "At minibatch 14700, batch loss 0.486372, batch error rate 14.000000%\n",
      "At minibatch 14800, batch loss 0.579090, batch error rate 13.000000%\n",
      "At minibatch 14900, batch loss 0.368524, batch error rate 12.000000%\n",
      "At minibatch 15000, batch loss 0.382469, batch error rate 11.000000%\n",
      "After epoch 30: valid_err_rate: 0.086300% currently going ot do 46 epochs\n",
      "At minibatch 15100, batch loss 0.435347, batch error rate 4.000000%\n",
      "At minibatch 15200, batch loss 0.363570, batch error rate 12.000000%\n",
      "At minibatch 15300, batch loss 0.268079, batch error rate 10.000000%\n",
      "At minibatch 15400, batch loss 0.304116, batch error rate 7.000000%\n",
      "At minibatch 15500, batch loss 0.307011, batch error rate 9.000000%\n",
      "After epoch 31: valid_err_rate: 0.084900% currently going ot do 47 epochs\n",
      "At minibatch 15600, batch loss 0.280618, batch error rate 5.000000%\n",
      "At minibatch 15700, batch loss 0.101599, batch error rate 4.000000%\n",
      "At minibatch 15800, batch loss 0.520721, batch error rate 11.000000%\n",
      "At minibatch 15900, batch loss 0.439788, batch error rate 13.000000%\n",
      "At minibatch 16000, batch loss 0.281015, batch error rate 6.000000%\n",
      "After epoch 32: valid_err_rate: 0.086100% currently going ot do 47 epochs\n",
      "At minibatch 16100, batch loss 0.259415, batch error rate 8.000000%\n",
      "At minibatch 16200, batch loss 0.371927, batch error rate 8.000000%\n",
      "At minibatch 16300, batch loss 0.184085, batch error rate 8.000000%\n",
      "At minibatch 16400, batch loss 0.223951, batch error rate 8.000000%\n",
      "At minibatch 16500, batch loss 0.152915, batch error rate 6.000000%\n",
      "After epoch 33: valid_err_rate: 0.084800% currently going ot do 50 epochs\n",
      "At minibatch 16600, batch loss 0.163585, batch error rate 4.000000%\n",
      "At minibatch 16700, batch loss 0.426133, batch error rate 13.000000%\n",
      "At minibatch 16800, batch loss 0.439422, batch error rate 15.000000%\n",
      "At minibatch 16900, batch loss 0.322832, batch error rate 7.000000%\n",
      "At minibatch 17000, batch loss 0.289676, batch error rate 9.000000%\n",
      "After epoch 34: valid_err_rate: 0.083800% currently going ot do 52 epochs\n",
      "At minibatch 17100, batch loss 0.132049, batch error rate 4.000000%\n",
      "At minibatch 17200, batch loss 0.422505, batch error rate 9.000000%\n",
      "At minibatch 17300, batch loss 0.277039, batch error rate 4.000000%\n",
      "At minibatch 17400, batch loss 0.265773, batch error rate 8.000000%\n",
      "At minibatch 17500, batch loss 0.349980, batch error rate 10.000000%\n",
      "After epoch 35: valid_err_rate: 0.083700% currently going ot do 53 epochs\n",
      "At minibatch 17600, batch loss 0.281444, batch error rate 5.000000%\n",
      "At minibatch 17700, batch loss 0.159421, batch error rate 4.000000%\n",
      "At minibatch 17800, batch loss 0.283990, batch error rate 13.000000%\n",
      "At minibatch 17900, batch loss 0.275241, batch error rate 9.000000%\n",
      "At minibatch 18000, batch loss 0.233352, batch error rate 7.000000%\n",
      "After epoch 36: valid_err_rate: 0.082700% currently going ot do 55 epochs\n",
      "At minibatch 18100, batch loss 0.175458, batch error rate 4.000000%\n",
      "At minibatch 18200, batch loss 0.440920, batch error rate 12.000000%\n",
      "At minibatch 18300, batch loss 0.208125, batch error rate 8.000000%\n",
      "At minibatch 18400, batch loss 0.329999, batch error rate 11.000000%\n",
      "At minibatch 18500, batch loss 0.435619, batch error rate 12.000000%\n",
      "After epoch 37: valid_err_rate: 0.083000% currently going ot do 55 epochs\n",
      "At minibatch 18600, batch loss 0.404513, batch error rate 12.000000%\n",
      "At minibatch 18700, batch loss 0.457838, batch error rate 8.000000%\n",
      "At minibatch 18800, batch loss 0.384474, batch error rate 10.000000%\n",
      "At minibatch 18900, batch loss 0.323384, batch error rate 9.000000%\n",
      "At minibatch 19000, batch loss 0.366607, batch error rate 9.000000%\n",
      "After epoch 38: valid_err_rate: 0.082300% currently going ot do 58 epochs\n",
      "At minibatch 19100, batch loss 0.235772, batch error rate 10.000000%\n",
      "At minibatch 19200, batch loss 0.617875, batch error rate 18.000000%\n",
      "At minibatch 19300, batch loss 0.376323, batch error rate 8.000000%\n",
      "At minibatch 19400, batch loss 0.315399, batch error rate 9.000000%\n",
      "At minibatch 19500, batch loss 0.243954, batch error rate 8.000000%\n",
      "After epoch 39: valid_err_rate: 0.083200% currently going ot do 58 epochs\n",
      "At minibatch 19600, batch loss 0.337743, batch error rate 12.000000%\n",
      "At minibatch 19700, batch loss 0.340325, batch error rate 11.000000%\n",
      "At minibatch 19800, batch loss 0.355968, batch error rate 9.000000%\n",
      "At minibatch 19900, batch loss 0.273670, batch error rate 11.000000%\n",
      "At minibatch 20000, batch loss 0.402254, batch error rate 5.000000%\n",
      "After epoch 40: valid_err_rate: 0.082700% currently going ot do 58 epochs\n",
      "At minibatch 20100, batch loss 0.282013, batch error rate 8.000000%\n",
      "At minibatch 20200, batch loss 0.378535, batch error rate 13.000000%\n",
      "At minibatch 20300, batch loss 0.353562, batch error rate 10.000000%\n",
      "At minibatch 20400, batch loss 0.373445, batch error rate 11.000000%\n",
      "At minibatch 20500, batch loss 0.420472, batch error rate 14.000000%\n",
      "After epoch 41: valid_err_rate: 0.083300% currently going ot do 58 epochs\n",
      "At minibatch 20600, batch loss 0.293581, batch error rate 8.000000%\n",
      "At minibatch 20700, batch loss 0.327987, batch error rate 7.000000%\n",
      "At minibatch 20800, batch loss 0.271201, batch error rate 10.000000%\n",
      "At minibatch 20900, batch loss 0.372514, batch error rate 12.000000%\n",
      "At minibatch 21000, batch loss 0.457439, batch error rate 11.000000%\n",
      "After epoch 42: valid_err_rate: 0.081900% currently going ot do 64 epochs\n",
      "At minibatch 21100, batch loss 0.502091, batch error rate 14.000000%\n",
      "At minibatch 21200, batch loss 0.437206, batch error rate 12.000000%\n",
      "At minibatch 21300, batch loss 0.190161, batch error rate 7.000000%\n",
      "At minibatch 21400, batch loss 0.137669, batch error rate 3.000000%\n",
      "At minibatch 21500, batch loss 0.290219, batch error rate 8.000000%\n",
      "After epoch 43: valid_err_rate: 0.082400% currently going ot do 64 epochs\n",
      "At minibatch 21600, batch loss 0.260735, batch error rate 7.000000%\n",
      "At minibatch 21700, batch loss 0.193775, batch error rate 5.000000%\n",
      "At minibatch 21800, batch loss 0.147423, batch error rate 5.000000%\n",
      "At minibatch 21900, batch loss 0.373346, batch error rate 10.000000%\n",
      "At minibatch 22000, batch loss 0.223587, batch error rate 6.000000%\n",
      "After epoch 44: valid_err_rate: 0.081300% currently going ot do 67 epochs\n",
      "At minibatch 22100, batch loss 0.596842, batch error rate 16.000000%\n",
      "At minibatch 22200, batch loss 0.326718, batch error rate 7.000000%\n",
      "At minibatch 22300, batch loss 0.234395, batch error rate 7.000000%\n",
      "At minibatch 22400, batch loss 0.180301, batch error rate 6.000000%\n",
      "At minibatch 22500, batch loss 0.205618, batch error rate 10.000000%\n",
      "After epoch 45: valid_err_rate: 0.080800% currently going ot do 68 epochs\n",
      "At minibatch 22600, batch loss 0.198924, batch error rate 5.000000%\n",
      "At minibatch 22700, batch loss 0.335538, batch error rate 13.000000%\n",
      "At minibatch 22800, batch loss 0.350549, batch error rate 10.000000%\n",
      "At minibatch 22900, batch loss 0.244206, batch error rate 5.000000%\n",
      "At minibatch 23000, batch loss 0.360949, batch error rate 13.000000%\n",
      "After epoch 46: valid_err_rate: 0.081200% currently going ot do 68 epochs\n",
      "At minibatch 23100, batch loss 0.250850, batch error rate 9.000000%\n",
      "At minibatch 23200, batch loss 0.376253, batch error rate 13.000000%\n",
      "At minibatch 23300, batch loss 0.275553, batch error rate 7.000000%\n",
      "At minibatch 23400, batch loss 0.286350, batch error rate 7.000000%\n",
      "At minibatch 23500, batch loss 0.356316, batch error rate 10.000000%\n",
      "After epoch 47: valid_err_rate: 0.080900% currently going ot do 68 epochs\n",
      "At minibatch 23600, batch loss 0.378738, batch error rate 11.000000%\n",
      "At minibatch 23700, batch loss 0.330497, batch error rate 6.000000%\n",
      "At minibatch 23800, batch loss 0.202839, batch error rate 7.000000%\n",
      "At minibatch 23900, batch loss 0.213022, batch error rate 6.000000%\n",
      "At minibatch 24000, batch loss 0.322637, batch error rate 8.000000%\n",
      "After epoch 48: valid_err_rate: 0.081500% currently going ot do 68 epochs\n",
      "At minibatch 24100, batch loss 0.459710, batch error rate 16.000000%\n",
      "At minibatch 24200, batch loss 0.424681, batch error rate 11.000000%\n",
      "At minibatch 24300, batch loss 0.301788, batch error rate 12.000000%\n",
      "At minibatch 24400, batch loss 0.279739, batch error rate 8.000000%\n",
      "At minibatch 24500, batch loss 0.467103, batch error rate 8.000000%\n",
      "After epoch 49: valid_err_rate: 0.080000% currently going ot do 74 epochs\n",
      "At minibatch 24600, batch loss 0.299301, batch error rate 9.000000%\n",
      "At minibatch 24700, batch loss 0.225233, batch error rate 9.000000%\n",
      "At minibatch 24800, batch loss 0.215603, batch error rate 6.000000%\n",
      "At minibatch 24900, batch loss 0.400818, batch error rate 12.000000%\n",
      "At minibatch 25000, batch loss 0.183707, batch error rate 6.000000%\n",
      "After epoch 50: valid_err_rate: 0.080700% currently going ot do 74 epochs\n",
      "At minibatch 25100, batch loss 0.231424, batch error rate 7.000000%\n",
      "At minibatch 25200, batch loss 0.375505, batch error rate 9.000000%\n",
      "At minibatch 25300, batch loss 0.331637, batch error rate 9.000000%\n",
      "At minibatch 25400, batch loss 0.224605, batch error rate 10.000000%\n",
      "At minibatch 25500, batch loss 0.353572, batch error rate 10.000000%\n",
      "After epoch 51: valid_err_rate: 0.080700% currently going ot do 74 epochs\n",
      "At minibatch 25600, batch loss 0.227622, batch error rate 11.000000%\n",
      "At minibatch 25700, batch loss 0.364920, batch error rate 8.000000%\n",
      "At minibatch 25800, batch loss 0.245654, batch error rate 8.000000%\n",
      "At minibatch 25900, batch loss 0.403081, batch error rate 17.000000%\n",
      "At minibatch 26000, batch loss 0.417729, batch error rate 10.000000%\n",
      "After epoch 52: valid_err_rate: 0.080400% currently going ot do 74 epochs\n",
      "At minibatch 26100, batch loss 0.272387, batch error rate 8.000000%\n",
      "At minibatch 26200, batch loss 0.226747, batch error rate 7.000000%\n",
      "At minibatch 26300, batch loss 0.213269, batch error rate 9.000000%\n",
      "At minibatch 26400, batch loss 0.327908, batch error rate 12.000000%\n",
      "At minibatch 26500, batch loss 0.374564, batch error rate 12.000000%\n",
      "After epoch 53: valid_err_rate: 0.080500% currently going ot do 74 epochs\n",
      "At minibatch 26600, batch loss 0.238657, batch error rate 7.000000%\n",
      "At minibatch 26700, batch loss 0.179457, batch error rate 6.000000%\n",
      "At minibatch 26800, batch loss 0.157800, batch error rate 6.000000%\n",
      "At minibatch 26900, batch loss 0.302052, batch error rate 11.000000%\n",
      "At minibatch 27000, batch loss 0.260533, batch error rate 5.000000%\n",
      "After epoch 54: valid_err_rate: 0.080500% currently going ot do 74 epochs\n",
      "At minibatch 27100, batch loss 0.317804, batch error rate 8.000000%\n",
      "At minibatch 27200, batch loss 0.235455, batch error rate 10.000000%\n",
      "At minibatch 27300, batch loss 0.273716, batch error rate 7.000000%\n",
      "At minibatch 27400, batch loss 0.435185, batch error rate 10.000000%\n",
      "At minibatch 27500, batch loss 0.336683, batch error rate 11.000000%\n",
      "After epoch 55: valid_err_rate: 0.079500% currently going ot do 83 epochs\n",
      "At minibatch 27600, batch loss 0.462934, batch error rate 12.000000%\n",
      "At minibatch 27700, batch loss 0.430652, batch error rate 11.000000%\n",
      "At minibatch 27800, batch loss 0.257238, batch error rate 6.000000%\n",
      "At minibatch 27900, batch loss 0.200000, batch error rate 6.000000%\n",
      "At minibatch 28000, batch loss 0.541948, batch error rate 13.000000%\n",
      "After epoch 56: valid_err_rate: 0.079300% currently going ot do 85 epochs\n",
      "At minibatch 28100, batch loss 0.247018, batch error rate 5.000000%\n",
      "At minibatch 28200, batch loss 0.270670, batch error rate 7.000000%\n",
      "At minibatch 28300, batch loss 0.229373, batch error rate 7.000000%\n",
      "At minibatch 28400, batch loss 0.237241, batch error rate 8.000000%\n",
      "At minibatch 28500, batch loss 0.296567, batch error rate 9.000000%\n",
      "After epoch 57: valid_err_rate: 0.078900% currently going ot do 86 epochs\n",
      "At minibatch 28600, batch loss 0.252645, batch error rate 5.000000%\n",
      "At minibatch 28700, batch loss 0.343324, batch error rate 7.000000%\n",
      "At minibatch 28800, batch loss 0.446589, batch error rate 10.000000%\n",
      "At minibatch 28900, batch loss 0.231637, batch error rate 8.000000%\n",
      "At minibatch 29000, batch loss 0.199392, batch error rate 5.000000%\n",
      "After epoch 58: valid_err_rate: 0.078200% currently going ot do 88 epochs\n",
      "At minibatch 29100, batch loss 0.289891, batch error rate 11.000000%\n",
      "At minibatch 29200, batch loss 0.173127, batch error rate 6.000000%\n",
      "At minibatch 29300, batch loss 0.365825, batch error rate 9.000000%\n",
      "At minibatch 29400, batch loss 0.359309, batch error rate 10.000000%\n",
      "At minibatch 29500, batch loss 0.285970, batch error rate 8.000000%\n",
      "After epoch 59: valid_err_rate: 0.078300% currently going ot do 88 epochs\n",
      "At minibatch 29600, batch loss 0.234272, batch error rate 7.000000%\n",
      "At minibatch 29700, batch loss 0.342483, batch error rate 10.000000%\n",
      "At minibatch 29800, batch loss 0.328376, batch error rate 9.000000%\n",
      "At minibatch 29900, batch loss 0.359963, batch error rate 10.000000%\n",
      "At minibatch 30000, batch loss 0.371305, batch error rate 12.000000%\n",
      "After epoch 60: valid_err_rate: 0.078000% currently going ot do 91 epochs\n",
      "At minibatch 30100, batch loss 0.331768, batch error rate 11.000000%\n",
      "At minibatch 30200, batch loss 0.184986, batch error rate 4.000000%\n",
      "At minibatch 30300, batch loss 0.361232, batch error rate 10.000000%\n",
      "At minibatch 30400, batch loss 0.221585, batch error rate 8.000000%\n",
      "At minibatch 30500, batch loss 0.271898, batch error rate 11.000000%\n",
      "After epoch 61: valid_err_rate: 0.078900% currently going ot do 91 epochs\n",
      "At minibatch 30600, batch loss 0.235679, batch error rate 7.000000%\n",
      "At minibatch 30700, batch loss 0.091747, batch error rate 1.000000%\n",
      "At minibatch 30800, batch loss 0.164547, batch error rate 4.000000%\n",
      "At minibatch 30900, batch loss 0.252826, batch error rate 6.000000%\n",
      "At minibatch 31000, batch loss 0.098367, batch error rate 2.000000%\n",
      "After epoch 62: valid_err_rate: 0.078600% currently going ot do 91 epochs\n",
      "At minibatch 31100, batch loss 0.304075, batch error rate 5.000000%\n",
      "At minibatch 31200, batch loss 0.289353, batch error rate 8.000000%\n",
      "At minibatch 31300, batch loss 0.270670, batch error rate 7.000000%\n",
      "At minibatch 31400, batch loss 0.270912, batch error rate 9.000000%\n",
      "At minibatch 31500, batch loss 0.276145, batch error rate 7.000000%\n",
      "After epoch 63: valid_err_rate: 0.077600% currently going ot do 95 epochs\n",
      "At minibatch 31600, batch loss 0.209494, batch error rate 5.000000%\n",
      "At minibatch 31700, batch loss 0.217888, batch error rate 7.000000%\n",
      "At minibatch 31800, batch loss 0.497166, batch error rate 13.000000%\n",
      "At minibatch 31900, batch loss 0.139032, batch error rate 7.000000%\n",
      "At minibatch 32000, batch loss 0.315155, batch error rate 7.000000%\n",
      "After epoch 64: valid_err_rate: 0.078000% currently going ot do 95 epochs\n",
      "At minibatch 32100, batch loss 0.359862, batch error rate 7.000000%\n",
      "At minibatch 32200, batch loss 0.272349, batch error rate 12.000000%\n",
      "At minibatch 32300, batch loss 0.339045, batch error rate 9.000000%\n",
      "At minibatch 32400, batch loss 0.177464, batch error rate 7.000000%\n",
      "At minibatch 32500, batch loss 0.222899, batch error rate 7.000000%\n",
      "After epoch 65: valid_err_rate: 0.078000% currently going ot do 95 epochs\n",
      "At minibatch 32600, batch loss 0.309851, batch error rate 11.000000%\n",
      "At minibatch 32700, batch loss 0.244391, batch error rate 9.000000%\n",
      "At minibatch 32800, batch loss 0.227754, batch error rate 7.000000%\n",
      "At minibatch 32900, batch loss 0.264332, batch error rate 7.000000%\n",
      "At minibatch 33000, batch loss 0.220375, batch error rate 9.000000%\n",
      "After epoch 66: valid_err_rate: 0.076800% currently going ot do 100 epochs\n",
      "At minibatch 33100, batch loss 0.241687, batch error rate 9.000000%\n",
      "At minibatch 33200, batch loss 0.221659, batch error rate 7.000000%\n",
      "At minibatch 33300, batch loss 0.300507, batch error rate 9.000000%\n",
      "At minibatch 33400, batch loss 0.284557, batch error rate 7.000000%\n",
      "At minibatch 33500, batch loss 0.290636, batch error rate 8.000000%\n",
      "After epoch 67: valid_err_rate: 0.076900% currently going ot do 100 epochs\n",
      "At minibatch 33600, batch loss 0.153961, batch error rate 3.000000%\n",
      "At minibatch 33700, batch loss 0.246200, batch error rate 7.000000%\n",
      "At minibatch 33800, batch loss 0.384276, batch error rate 13.000000%\n",
      "At minibatch 33900, batch loss 0.113472, batch error rate 2.000000%\n",
      "At minibatch 34000, batch loss 0.480429, batch error rate 8.000000%\n",
      "After epoch 68: valid_err_rate: 0.077600% currently going ot do 100 epochs\n",
      "At minibatch 34100, batch loss 0.451492, batch error rate 9.000000%\n",
      "At minibatch 34200, batch loss 0.204232, batch error rate 9.000000%\n",
      "At minibatch 34300, batch loss 0.117250, batch error rate 5.000000%\n",
      "At minibatch 34400, batch loss 0.363913, batch error rate 11.000000%\n",
      "At minibatch 34500, batch loss 0.265388, batch error rate 8.000000%\n",
      "After epoch 69: valid_err_rate: 0.076800% currently going ot do 100 epochs\n",
      "At minibatch 34600, batch loss 0.155884, batch error rate 4.000000%\n",
      "At minibatch 34700, batch loss 0.294176, batch error rate 15.000000%\n",
      "At minibatch 34800, batch loss 0.353251, batch error rate 9.000000%\n",
      "At minibatch 34900, batch loss 0.402484, batch error rate 12.000000%\n",
      "At minibatch 35000, batch loss 0.401744, batch error rate 11.000000%\n",
      "After epoch 70: valid_err_rate: 0.077200% currently going ot do 100 epochs\n",
      "At minibatch 35100, batch loss 0.275725, batch error rate 9.000000%\n",
      "At minibatch 35200, batch loss 0.367354, batch error rate 11.000000%\n",
      "At minibatch 35300, batch loss 0.375762, batch error rate 7.000000%\n",
      "At minibatch 35400, batch loss 0.237400, batch error rate 6.000000%\n",
      "At minibatch 35500, batch loss 0.249940, batch error rate 10.000000%\n",
      "After epoch 71: valid_err_rate: 0.076900% currently going ot do 100 epochs\n",
      "At minibatch 35600, batch loss 0.226326, batch error rate 6.000000%\n",
      "At minibatch 35700, batch loss 0.262776, batch error rate 5.000000%\n",
      "At minibatch 35800, batch loss 0.168079, batch error rate 4.000000%\n",
      "At minibatch 35900, batch loss 0.279686, batch error rate 8.000000%\n",
      "At minibatch 36000, batch loss 0.211971, batch error rate 9.000000%\n",
      "After epoch 72: valid_err_rate: 0.076400% currently going ot do 109 epochs\n",
      "At minibatch 36100, batch loss 0.457654, batch error rate 13.000000%\n",
      "At minibatch 36200, batch loss 0.161125, batch error rate 3.000000%\n",
      "At minibatch 36300, batch loss 0.180649, batch error rate 4.000000%\n",
      "At minibatch 36400, batch loss 0.138601, batch error rate 6.000000%\n",
      "At minibatch 36500, batch loss 0.188434, batch error rate 4.000000%\n",
      "After epoch 73: valid_err_rate: 0.076100% currently going ot do 110 epochs\n",
      "At minibatch 36600, batch loss 0.186684, batch error rate 8.000000%\n",
      "At minibatch 36700, batch loss 0.310395, batch error rate 10.000000%\n",
      "At minibatch 36800, batch loss 0.244936, batch error rate 7.000000%\n",
      "At minibatch 36900, batch loss 0.167359, batch error rate 7.000000%\n",
      "At minibatch 37000, batch loss 0.419158, batch error rate 11.000000%\n",
      "After epoch 74: valid_err_rate: 0.076900% currently going ot do 110 epochs\n",
      "At minibatch 37100, batch loss 0.384244, batch error rate 10.000000%\n",
      "At minibatch 37200, batch loss 0.240151, batch error rate 8.000000%\n",
      "At minibatch 37300, batch loss 0.371127, batch error rate 10.000000%\n",
      "At minibatch 37400, batch loss 0.215719, batch error rate 7.000000%\n",
      "At minibatch 37500, batch loss 0.330762, batch error rate 9.000000%\n",
      "Setting network parameters from after epoch 73\n",
      "Test error rate: 0.080500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFnCAYAAAAGxCvgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28cae1f3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0744"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3 [2p bonus]\n",
    "\n",
    "Implement norm constraints, i.e. limit the total\n",
    "norm of connections incoming to a neuron. In our case, this\n",
    "corresponds to clipping the norm of *rows* of weight\n",
    "matrices. An easy way of implementing it is to make a gradient\n",
    "step, then look at the norm of rows and scale down those that are\n",
    "over the threshold (this technique is called \"projected gradient descent\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [2p bonus]\n",
    "\n",
    "Implement a **dropout** layer and try to train a\n",
    "network getting below 1.5% test error rates with dropout (the best\n",
    "result is below 1\\% for dropout!). Details: http://arxiv.org/pdf/1207.0580.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 [3p bonus]\n",
    "\n",
    "Implement convolutional and max-pooling layers and (without dropout) get a test error rate below 1.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 [1-3p bonus]\n",
    "\n",
    "Implement a data augmentation method (e.g. rotations, noise, crops) that will yield a significant test error rate reduction for your network. Number of bonus points depends on the ingenuity of your solution and error rate gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
